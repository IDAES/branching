{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show how to use the branching package to learn branching rules that approximate the strong branching in the set covering problem, which can be applied to any problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import necessary packages and functions from the branching package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import argparse\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ecole\n",
    "import natsort\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branching\n",
    "from branching.utilities import log\n",
    "from branching.sampling import collect_samples\n",
    "from branching.feature_names import khalil_feat_names, node_candidate_feat_names\n",
    "from branching.create_datasets import load_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "\n",
    "In this stage, we will solve training and validation instances and collect measurements.\n",
    "\n",
    "Select the set of training and validation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train = glob.glob(f\"instances-setcover/train/*.lp\")\n",
    "instances_valid = glob.glob(f\"instances-setcover/valid/*.lp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of processors (njobs) to solve instances in parallel.\n",
    "The seed determines:\n",
    "1. The order in which instances will be solved.\n",
    "2. Additional seeds assigned to each instance and fed to the solver that is used for permuting the rows/columns of an instance and the underlying LP solver.\n",
    "3. The subproblems that will be stored as data.\n",
    "\n",
    "A seed uniquely determines the data to be collected and as a result, the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs=5\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "logfile = f'logs/setcover.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue sampling until we collect 5000 candidate measurements. To make sure that we collect data from different instances, we store a subproblem and perform strong branching at it with 5% probability and impose a node limit of 1000 per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncands_size = 5000\n",
    "node_record_prob = 0.05\n",
    "node_limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 20:26:19.774445] Node record probability: 0.05\n",
      "[2023-09-13 20:26:19.775902] Candidate limit (total): 5000\n",
      "[2023-09-13 20:26:19.776386] Node limit (per instance): 1000\n"
     ]
    }
   ],
   "source": [
    "out_dir = f\"data/setcover/samples/{seed}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "log(f\"Node record probability: {node_record_prob}\", logfile)\n",
    "log(f\"Candidate limit (total): {ncands_size}\", logfile)\n",
    "log(f\"Node limit (per instance): {node_limit}\", logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve training instances and log some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 20:26:19.800760] 20 training instances to collect 5000 candidates\n",
      "[w 8258] episode 0, seed 686441525, processing instance 'instances-setcover/train/instance_6.lp'...\n",
      "[w 8260] episode 1, seed 4022173117, processing instance 'instances-setcover/train/instance_3.lp'...\n",
      "[w 8257] episode 2, seed 92199959, processing instance 'instances-setcover/train/instance_6.lp'...\n",
      "[w 8261] episode 3, seed 2988243274, processing instance 'instances-setcover/train/instance_4.lp'...\n",
      "[w 8259] episode 4, seed 478168918, processing instance 'instances-setcover/train/instance_17.lp'...\n",
      "[m 8233] 1 samples written, 93 cand.s collected, ep 0 (3 in buffer).\n",
      "[m 8233] 2 samples written, 182 cand.s collected, ep 0 (5 in buffer).\n",
      "[m 8233] 3 samples written, 268 cand.s collected, ep 0 (6 in buffer).\n",
      "[w 8258] episode 0 done, 3 samples\n",
      "[w 8258] episode 5, seed 1570198172, processing instance 'instances-setcover/train/instance_9.lp'...\n",
      "[m 8233] 4 samples written, 381 cand.s collected, ep 1 (12 in buffer).\n",
      "[w 8261] episode 3 done, 3 samples\n",
      "[w 8261] episode 6, seed 3582236023, processing instance 'instances-setcover/train/instance_19.lp'...\n",
      "[w 8259] episode 4 done, 4 samples\n",
      "[w 8259] episode 7, seed 829993470, processing instance 'instances-setcover/train/instance_2.lp'...\n",
      "[w 8261] episode 6 done, 1 samples\n",
      "[w 8261] episode 8, seed 547450770, processing instance 'instances-setcover/train/instance_19.lp'...\n",
      "[w 8257] episode 2 done, 9 samples\n",
      "[m 8233] 5 samples written, 457 cand.s collected, ep 1 (17 in buffer).\n",
      "[w 8257] episode 9, seed 3670656296, processing instance 'instances-setcover/train/instance_11.lp'...\n",
      "[m 8233] 6 samples written, 530 cand.s collected, ep 1 (18 in buffer).\n",
      "[w 8261] episode 8 done, 1 samples\n",
      "[w 8261] episode 10, seed 1056212150, processing instance 'instances-setcover/train/instance_9.lp'...\n",
      "[m 8233] 7 samples written, 636 cand.s collected, ep 1 (20 in buffer).\n",
      "[w 8259] episode 7 done, 1 samples\n",
      "[w 8259] episode 11, seed 2653764935, processing instance 'instances-setcover/train/instance_10.lp'...\n",
      "[m 8233] 8 samples written, 723 cand.s collected, ep 1 (23 in buffer).\n",
      "[m 8233] 9 samples written, 822 cand.s collected, ep 1 (27 in buffer).\n",
      "[m 8233] 10 samples written, 920 cand.s collected, ep 1 (33 in buffer).\n",
      "[m 8233] 11 samples written, 999 cand.s collected, ep 1 (38 in buffer).\n",
      "[m 8233] 12 samples written, 1099 cand.s collected, ep 1 (40 in buffer).\n",
      "[m 8233] 13 samples written, 1193 cand.s collected, ep 1 (43 in buffer).\n",
      "[w 8260] episode 1 done, 10 samples\n",
      "[m 8233] 14 samples written, 1283 cand.s collected, ep 2 (42 in buffer).\n",
      "[m 8233] 15 samples written, 1375 cand.s collected, ep 2 (41 in buffer).\n",
      "[m 8233] 16 samples written, 1460 cand.s collected, ep 2 (40 in buffer).\n",
      "[m 8233] 17 samples written, 1538 cand.s collected, ep 2 (39 in buffer).\n",
      "[m 8233] 18 samples written, 1613 cand.s collected, ep 2 (38 in buffer).\n",
      "[m 8233] 19 samples written, 1692 cand.s collected, ep 2 (37 in buffer).\n",
      "[m 8233] 20 samples written, 1779 cand.s collected, ep 2 (36 in buffer).\n",
      "[m 8233] 21 samples written, 1860 cand.s collected, ep 2 (35 in buffer).\n",
      "[m 8233] 22 samples written, 1930 cand.s collected, ep 2 (34 in buffer).\n",
      "[m 8233] 23 samples written, 2016 cand.s collected, ep 3 (33 in buffer).\n",
      "[m 8233] 24 samples written, 2085 cand.s collected, ep 3 (32 in buffer).\n",
      "[m 8233] 25 samples written, 2165 cand.s collected, ep 3 (31 in buffer).\n",
      "[m 8233] 26 samples written, 2263 cand.s collected, ep 4 (30 in buffer).\n",
      "[m 8233] 27 samples written, 2359 cand.s collected, ep 4 (29 in buffer).\n",
      "[m 8233] 28 samples written, 2451 cand.s collected, ep 4 (28 in buffer).\n",
      "[m 8233] 29 samples written, 2538 cand.s collected, ep 4 (27 in buffer).\n",
      "[m 8233] 30 samples written, 2647 cand.s collected, ep 5 (26 in buffer).\n",
      "[m 8233] 31 samples written, 2730 cand.s collected, ep 5 (25 in buffer).\n",
      "[m 8233] 32 samples written, 2831 cand.s collected, ep 5 (24 in buffer).\n",
      "[m 8233] 33 samples written, 2922 cand.s collected, ep 5 (23 in buffer).\n",
      "[m 8233] 34 samples written, 3015 cand.s collected, ep 5 (22 in buffer).\n",
      "[w 8260] episode 12, seed 1240388383, processing instance 'instances-setcover/train/instance_2.lp'...\n",
      "[m 8233] 35 samples written, 3109 cand.s collected, ep 5 (24 in buffer).\n",
      "[m 8233] 36 samples written, 3201 cand.s collected, ep 5 (26 in buffer).\n",
      "[m 8233] 37 samples written, 3282 cand.s collected, ep 5 (27 in buffer).\n",
      "[m 8233] 38 samples written, 3373 cand.s collected, ep 5 (27 in buffer).\n",
      "[w 8260] episode 12 done, 1 samples\n",
      "[w 8260] episode 13, seed 4003263624, processing instance 'instances-setcover/train/instance_6.lp'...\n",
      "[w 8259] episode 11 done, 8 samples\n",
      "[w 8259] episode 14, seed 1585552260, processing instance 'instances-setcover/train/instance_13.lp'...\n",
      "[m 8233] 39 samples written, 3444 cand.s collected, ep 5 (33 in buffer).\n",
      "[w 8257] episode 9 done, 17 samples\n",
      "[w 8257] episode 15, seed 2390286921, processing instance 'instances-setcover/train/instance_20.lp'...\n",
      "[w 8259] episode 14 done, 2 samples\n",
      "[w 8259] episode 16, seed 963517372, processing instance 'instances-setcover/train/instance_4.lp'...\n",
      "[m 8233] 40 samples written, 3511 cand.s collected, ep 5 (44 in buffer).\n",
      "[w 8257] episode 15 done, 1 samples\n",
      "[w 8257] episode 17, seed 1853550781, processing instance 'instances-setcover/train/instance_12.lp'...\n",
      "[m 8233] 41 samples written, 3585 cand.s collected, ep 5 (44 in buffer).\n",
      "[w 8260] episode 13 done, 6 samples\n",
      "[w 8260] episode 18, seed 2110980796, processing instance 'instances-setcover/train/instance_15.lp'...\n",
      "[m 8233] 42 samples written, 3686 cand.s collected, ep 5 (52 in buffer).\n",
      "[w 8259] episode 16 done, 5 samples\n",
      "[w 8259] episode 19, seed 3622480696, processing instance 'instances-setcover/train/instance_17.lp'...\n",
      "[w 8260] episode 18 done, 2 samples\n",
      "[w 8260] episode 20, seed 1307717426, processing instance 'instances-setcover/train/instance_12.lp'...\n",
      "[m 8233] 43 samples written, 3775 cand.s collected, ep 5 (60 in buffer).\n",
      "[m 8233] 44 samples written, 3838 cand.s collected, ep 5 (60 in buffer).\n",
      "[m 8233] 45 samples written, 3917 cand.s collected, ep 5 (62 in buffer).\n",
      "[m 8233] 46 samples written, 4011 cand.s collected, ep 5 (62 in buffer).\n",
      "[m 8233] 47 samples written, 4077 cand.s collected, ep 5 (62 in buffer).\n",
      "[m 8233] 48 samples written, 4168 cand.s collected, ep 5 (65 in buffer).\n",
      "[m 8233] 49 samples written, 4256 cand.s collected, ep 5 (68 in buffer).\n",
      "[w 8259] episode 19 done, 3 samples\n",
      "[w 8259] episode 21, seed 1432520389, processing instance 'instances-setcover/train/instance_4.lp'...\n",
      "[m 8233] 50 samples written, 4348 cand.s collected, ep 5 (74 in buffer).\n",
      "[w 8259] episode 21 done, 6 samples\n",
      "[w 8259] episode 22, seed 1798177303, processing instance 'instances-setcover/train/instance_2.lp'...\n",
      "[m 8233] 51 samples written, 4428 cand.s collected, ep 5 (92 in buffer).\n",
      "[m 8233] 52 samples written, 4506 cand.s collected, ep 5 (92 in buffer).\n",
      "[w 8259] episode 22 done, 1 samples\n",
      "[w 8259] episode 23, seed 1901913596, processing instance 'instances-setcover/train/instance_18.lp'...\n",
      "[m 8233] 53 samples written, 4598 cand.s collected, ep 5 (102 in buffer).\n",
      "[m 8233] 54 samples written, 4682 cand.s collected, ep 5 (102 in buffer).\n",
      "[w 8261] episode 10 done, 23 samples\n",
      "[w 8261] episode 24, seed 2884627899, processing instance 'instances-setcover/train/instance_20.lp'...\n",
      "[w 8259] episode 23 done, 1 samples\n",
      "[w 8259] episode 25, seed 1280629596, processing instance 'instances-setcover/train/instance_6.lp'...\n",
      "[m 8233] 55 samples written, 4767 cand.s collected, ep 5 (104 in buffer).\n",
      "[m 8233] 56 samples written, 4847 cand.s collected, ep 5 (106 in buffer).\n",
      "[w 8261] episode 24 done, 2 samples\n",
      "[w 8261] episode 26, seed 1020859476, processing instance 'instances-setcover/train/instance_16.lp'...\n",
      "[m 8233] 57 samples written, 4915 cand.s collected, ep 5 (110 in buffer).\n",
      "[w 8261] episode 26 done, 3 samples\n",
      "[w 8261] episode 27, seed 705543863, processing instance 'instances-setcover/train/instance_4.lp'...\n",
      "[w 8259] episode 25 done, 2 samples\n",
      "[w 8259] episode 28, seed 2322133788, processing instance 'instances-setcover/train/instance_9.lp'...\n",
      "[w 8257] episode 17 done, 16 samples\n",
      "[w 8257] episode 29, seed 3646743884, processing instance 'instances-setcover/train/instance_19.lp'...\n",
      "[m 8233] 58 samples written, 4998 cand.s collected, ep 5 (120 in buffer).\n",
      "[w 8260] episode 20 done, 17 samples\n",
      "[w 8260] episode 30, seed 4203421552, processing instance 'instances-setcover/train/instance_17.lp'...\n",
      "[w 8257] episode 29 done, 0 samples\n",
      "[w 8257] episode 31, seed 3203036434, processing instance 'instances-setcover/train/instance_3.lp'...\n",
      "[w 8261] episode 27 done, 4 samples\n",
      "[w 8261] episode 32, seed 3583570687, processing instance 'instances-setcover/train/instance_5.lp'...\n",
      "[m 8233] 59 samples written, 5078 cand.s collected, ep 5 (125 in buffer).\n",
      "[m 8233] dispatcher stopped...\n",
      "[2023-09-13 20:31:40.392179] Training samples: --- 320.59 seconds ---\n",
      "[2023-09-13 20:31:40.405134] Number of episodes: 6, Number of samples: 59, Number of candidates: 5078\n"
     ]
    }
   ],
   "source": [
    "log(f\"{len(instances_train)} training instances to collect {ncands_size} candidates\", logfile)\n",
    "\n",
    "instances_train = natsort.natsorted(instances_train)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "nepisodes, nsamples, ncands = collect_samples(instances_train, out_dir + '/train', rng, ncands_size,\n",
    "            njobs, query_expert_prob=node_record_prob,\n",
    "            node_limit=node_limit)\n",
    "log(\"Training samples: --- %.2f seconds ---\" % (time.time() - start_time), logfile)\n",
    "log(f\"Number of episodes: {nepisodes}, Number of samples: {nsamples}, Number of candidates: {ncands}\", logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we had to solve 6 instances. 59 subproblems were stored collectively. In total, we have 5078 candidate variable measurements.\n",
    "\n",
    "Similarly, we solve validation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 20:31:40.580746] 20 validation instances to collect 5000 candidates\n",
      "[w 8708] episode 0, seed 686441525, processing instance 'instances-setcover/valid/instance_6.lp'...\n",
      "[w 8705] episode 2, seed 92199959, processing instance 'instances-setcover/valid/instance_6.lp'...\n",
      "[w 8706] episode 1, seed 4022173117, processing instance 'instances-setcover/valid/instance_3.lp'...\n",
      "[w 8704] episode 3, seed 2988243274, processing instance 'instances-setcover/valid/instance_4.lp'...\n",
      "[w 8707] episode 4, seed 478168918, processing instance 'instances-setcover/valid/instance_17.lp'...\n",
      "[w 8707] episode 4 done, 2 samples\n",
      "[w 8707] episode 5, seed 1570198172, processing instance 'instances-setcover/valid/instance_9.lp'...\n",
      "[m 8233] 1 samples written, 83 cand.s collected, ep 0 (7 in buffer).\n",
      "[m 8233] 2 samples written, 175 cand.s collected, ep 0 (9 in buffer).\n",
      "[m 8233] 3 samples written, 262 cand.s collected, ep 0 (10 in buffer).\n",
      "[m 8233] 4 samples written, 358 cand.s collected, ep 0 (27 in buffer).\n",
      "[m 8233] 5 samples written, 455 cand.s collected, ep 0 (29 in buffer).\n",
      "[m 8233] 6 samples written, 533 cand.s collected, ep 0 (29 in buffer).\n",
      "[m 8233] 7 samples written, 619 cand.s collected, ep 0 (40 in buffer).\n",
      "[m 8233] 8 samples written, 705 cand.s collected, ep 0 (45 in buffer).\n",
      "[m 8233] 9 samples written, 784 cand.s collected, ep 0 (56 in buffer).\n",
      "[m 8233] 10 samples written, 871 cand.s collected, ep 0 (69 in buffer).\n",
      "[w 8706] episode 1 done, 17 samples\n",
      "[w 8706] episode 6, seed 3582236023, processing instance 'instances-setcover/valid/instance_19.lp'...\n",
      "[m 8233] 11 samples written, 971 cand.s collected, ep 0 (76 in buffer).\n",
      "[w 8705] episode 2 done, 19 samples\n",
      "[w 8705] episode 7, seed 829993470, processing instance 'instances-setcover/valid/instance_2.lp'...\n",
      "[m 8233] 12 samples written, 1068 cand.s collected, ep 0 (87 in buffer).\n",
      "[w 8704] episode 3 done, 22 samples\n",
      "[w 8704] episode 8, seed 547450770, processing instance 'instances-setcover/valid/instance_19.lp'...\n",
      "[m 8233] 13 samples written, 1157 cand.s collected, ep 0 (90 in buffer).\n",
      "[m 8233] 14 samples written, 1251 cand.s collected, ep 0 (94 in buffer).\n",
      "[m 8233] 15 samples written, 1337 cand.s collected, ep 0 (95 in buffer).\n",
      "[m 8233] 16 samples written, 1418 cand.s collected, ep 0 (97 in buffer).\n",
      "[m 8233] 17 samples written, 1511 cand.s collected, ep 0 (100 in buffer).\n",
      "[w 8705] episode 7 done, 8 samples\n",
      "[w 8705] episode 9, seed 3670656296, processing instance 'instances-setcover/valid/instance_11.lp'...\n",
      "[m 8233] 18 samples written, 1605 cand.s collected, ep 0 (107 in buffer).\n",
      "[w 8708] episode 0 done, 18 samples\n",
      "[m 8233] 19 samples written, 1693 cand.s collected, ep 1 (108 in buffer).\n",
      "[m 8233] 20 samples written, 1782 cand.s collected, ep 1 (107 in buffer).\n",
      "[m 8233] 21 samples written, 1862 cand.s collected, ep 1 (106 in buffer).\n",
      "[m 8233] 22 samples written, 1949 cand.s collected, ep 1 (105 in buffer).\n",
      "[m 8233] 23 samples written, 2025 cand.s collected, ep 1 (104 in buffer).\n",
      "[m 8233] 24 samples written, 2102 cand.s collected, ep 1 (103 in buffer).\n",
      "[m 8233] 25 samples written, 2178 cand.s collected, ep 1 (102 in buffer).\n",
      "[m 8233] 26 samples written, 2262 cand.s collected, ep 1 (101 in buffer).\n",
      "[m 8233] 27 samples written, 2339 cand.s collected, ep 1 (100 in buffer).\n",
      "[m 8233] 28 samples written, 2409 cand.s collected, ep 1 (99 in buffer).\n",
      "[m 8233] 29 samples written, 2501 cand.s collected, ep 1 (98 in buffer).\n",
      "[m 8233] 30 samples written, 2594 cand.s collected, ep 1 (97 in buffer).\n",
      "[m 8233] 31 samples written, 2700 cand.s collected, ep 1 (96 in buffer).\n",
      "[m 8233] 32 samples written, 2792 cand.s collected, ep 1 (95 in buffer).\n",
      "[m 8233] 33 samples written, 2883 cand.s collected, ep 1 (94 in buffer).\n",
      "[m 8233] 34 samples written, 2978 cand.s collected, ep 1 (93 in buffer).\n",
      "[m 8233] 35 samples written, 3061 cand.s collected, ep 1 (92 in buffer).\n",
      "[m 8233] 36 samples written, 3161 cand.s collected, ep 2 (91 in buffer).\n",
      "[m 8233] 37 samples written, 3266 cand.s collected, ep 2 (90 in buffer).\n",
      "[m 8233] 38 samples written, 3354 cand.s collected, ep 2 (89 in buffer).\n",
      "[m 8233] 39 samples written, 3441 cand.s collected, ep 2 (88 in buffer).\n",
      "[m 8233] 40 samples written, 3519 cand.s collected, ep 2 (87 in buffer).\n",
      "[m 8233] 41 samples written, 3593 cand.s collected, ep 2 (86 in buffer).\n",
      "[m 8233] 42 samples written, 3684 cand.s collected, ep 2 (85 in buffer).\n",
      "[m 8233] 43 samples written, 3772 cand.s collected, ep 2 (84 in buffer).\n",
      "[m 8233] 44 samples written, 3867 cand.s collected, ep 2 (83 in buffer).\n",
      "[m 8233] 45 samples written, 3946 cand.s collected, ep 2 (82 in buffer).\n",
      "[m 8233] 46 samples written, 4022 cand.s collected, ep 2 (81 in buffer).\n",
      "[m 8233] 47 samples written, 4110 cand.s collected, ep 2 (80 in buffer).\n",
      "[m 8233] 48 samples written, 4199 cand.s collected, ep 2 (79 in buffer).\n",
      "[m 8233] 49 samples written, 4269 cand.s collected, ep 2 (78 in buffer).\n",
      "[m 8233] 50 samples written, 4352 cand.s collected, ep 2 (77 in buffer).\n",
      "[m 8233] 51 samples written, 4447 cand.s collected, ep 2 (76 in buffer).\n",
      "[m 8233] 52 samples written, 4549 cand.s collected, ep 2 (75 in buffer).\n",
      "[m 8233] 53 samples written, 4639 cand.s collected, ep 2 (74 in buffer).\n",
      "[m 8233] 54 samples written, 4737 cand.s collected, ep 2 (73 in buffer).\n",
      "[m 8233] 55 samples written, 4821 cand.s collected, ep 3 (72 in buffer).\n",
      "[m 8233] 56 samples written, 4886 cand.s collected, ep 3 (71 in buffer).\n",
      "[m 8233] 57 samples written, 4964 cand.s collected, ep 3 (70 in buffer).\n",
      "[m 8233] 58 samples written, 5047 cand.s collected, ep 3 (69 in buffer).\n",
      "[m 8233] dispatcher stopped...\n",
      "[2023-09-13 20:40:57.828551] Validation samples: --- 557.24 seconds ---\n",
      "[2023-09-13 20:40:57.832457] Number of episodes: 4, Number of samples: 58, Number of candidates: 5047\n"
     ]
    }
   ],
   "source": [
    "log(f\"{len(instances_valid)} validation instances to collect {ncands_size} candidates\", logfile)\n",
    "\n",
    "instances_valid = natsort.natsorted(instances_valid)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "nepisodes, nsamples, ncands = collect_samples(instances_valid, out_dir + '/valid', rng, ncands_size,\n",
    "                    njobs, query_expert_prob=node_record_prob,\n",
    "                    node_limit=node_limit)\n",
    "\n",
    "log(\"Validation samples: --- %.2f seconds ---\" % (time.time() - start_time), logfile)\n",
    "log(f\"Number of episodes: {nepisodes}, Number of samples: {nsamples}, Number of candidates: {ncands}\", logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_candidate_feat_names_dict = node_candidate_feat_names()\n",
    "node_candidate_feats = []\n",
    "    \n",
    "khalil_feat_names_dict = khalil_feat_names()\n",
    "khalil_feats = []\n",
    "\n",
    "for i in range(len(node_candidate_feat_names_dict)):\n",
    "    node_candidate_feats.append(f\"N_{i+1}_{node_candidate_feat_names_dict[i]}\")\n",
    "    \n",
    "for i in range(len(khalil_feat_names_dict)):\n",
    "    khalil_feats.append(f\"K_{i+1}_{khalil_feat_names_dict[i]}\")\n",
    "\n",
    "feat_names = node_candidate_feats + khalil_feats # combine dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = list(pathlib.Path(f'data/setcover/samples/{seed}/train').glob('sample_*.pkl'))\n",
    "valid_files = list(pathlib.Path(f'data/setcover/samples/{seed}/valid').glob('sample_*.pkl'))\n",
    "\n",
    "train_files = natsort.natsorted(train_files)\n",
    "valid_files = natsort.natsorted(valid_files)\n",
    "\n",
    "train_max_size = ncands_size\n",
    "valid_max_size = ncands_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the first 5000 candidate measurements obtained by solving training (validation) instances into a training (validation) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 20:42:36.895312] Loading training samples\n",
      "[2023-09-13 20:42:37.018086]   dataset size limit reached (5000 candidate variables)\n",
      "[2023-09-13 20:42:37.022095]   5000 training samples\n",
      "[2023-09-13 20:42:37.022335] Loading validation samples\n",
      "[2023-09-13 20:42:37.124208]   dataset size limit reached (5000 candidate variables)\n",
      "[2023-09-13 20:42:37.157290]   5000 validation samples\n"
     ]
    }
   ],
   "source": [
    "log(\"Loading training samples\", logfile)\n",
    "\n",
    "train_x, train_y, train_ncands = load_samples(\n",
    "        train_files, train_max_size, logfile)\n",
    "log(f\"  {train_x.shape[0]} training samples\", logfile)\n",
    "\n",
    "log(\"Loading validation samples\", logfile)\n",
    "\n",
    "valid_x, valid_y, valid_ncands = load_samples(\n",
    "        valid_files, valid_max_size, logfile)\n",
    "\n",
    "log(f\"  {valid_x.shape[0]} validation samples\", logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = np.concatenate((feat_names, [\"Score\"]))\n",
    "\n",
    "train_dataset = np.concatenate((train_x, train_y.reshape(-1,1)), axis = 1)\n",
    "valid_dataset = np.concatenate((valid_x, valid_y.reshape(-1,1)), axis = 1)\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset, columns = col_names)\n",
    "valid_df = pd.DataFrame(valid_dataset, columns = col_names)\n",
    "\n",
    "os.makedirs(f\"data/setcover/datasets/{seed}\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(f\"data/setcover/datasets/{seed}/train.csv\", index = False)\n",
    "valid_df.to_csv(f\"data/setcover/datasets/{seed}/valid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing datasets\n",
    "\n",
    "We eliminate constant features and highly correlated features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 20:42:44.823032] Before preprocessing:  110 features\n",
      "[2023-09-13 20:42:44.986759] After preprocessing:  65 features\n",
      "[2023-09-13 20:42:44.987020] Features: \n",
      "[2023-09-13 20:42:44.987754] Index(['N_1_objective', 'N_9_solution_value', 'N_14_incumbent_value',\n",
      "       'N_15_average_incumbent_value', 'N_20_solution_infeasibility',\n",
      "       'N_21_edge_mean', 'N_22_edge_min', 'N_23_edge_max', 'N_24_bias_mean',\n",
      "       'N_25_bias_min', 'N_26_bias_max', 'N_27_obj_cos_sim_mean',\n",
      "       'N_28_obj_cos_sim_min', 'N_29_obj_cos_sim_max', 'N_30_is_tight_mean',\n",
      "       'N_33_dual_solution_mean', 'N_34_dual_solution_min',\n",
      "       'N_36_scaled_age_mean', 'N_38_scaled_age_max', 'K_1_obj_coef',\n",
      "       'K_4_n_rows', 'K_5_rows_deg_mean', 'K_6_rows_deg_stddev',\n",
      "       'K_7_rows_deg_min', 'K_9_rows_pos_coefs_count',\n",
      "       'K_14_rows_neg_coefs_count', 'K_15_rows_neg_coefs_mean',\n",
      "       'K_16_rows_neg_coefs_stddev', 'K_17_rows_neg_coefs_min',\n",
      "       'K_18_rows_neg_coefs_max', 'K_21_pseudocost_up', 'K_22_pseudocost_down',\n",
      "       'K_23_pseudocost_ratio', 'K_24_pseudocost_sum',\n",
      "       'K_25_pseudocost_product', 'K_26_n_cutoff_up', 'K_27_n_cutoff_down',\n",
      "       'K_28_n_cutoff_up_ratio', 'K_29_n_cutoff_down_ratio',\n",
      "       'K_32_rows_dynamic_deg_min', 'K_34_rows_dynamic_deg_mean_ratio',\n",
      "       'K_35_rows_dynamic_deg_min_ratio', 'K_36_rows_dynamic_deg_max_ratio',\n",
      "       'K_37_coef_pos_rhs_ratio_min', 'K_38_coef_pos_rhs_ratio_max',\n",
      "       'K_41_pos_coef_pos_coef_ratio_min', 'K_42_pos_coef_pos_coef_ratio_max',\n",
      "       'K_48_neg_coef_neg_coef_ratio_max', 'K_49_active_coef_weight1_count',\n",
      "       'K_51_active_coef_weight1_mean', 'K_52_active_coef_weight1_stddev',\n",
      "       'K_53_active_coef_weight1_min', 'K_54_active_coef_weight1_max',\n",
      "       'K_57_active_coef_weight2_mean', 'K_58_active_coef_weight2_stddev',\n",
      "       'K_59_active_coef_weight2_min', 'K_60_active_coef_weight2_max',\n",
      "       'K_63_active_coef_weight3_mean', 'K_64_active_coef_weight3_stddev',\n",
      "       'K_65_active_coef_weight3_min', 'K_66_active_coef_weight3_max',\n",
      "       'K_69_active_coef_weight4_mean', 'K_70_active_coef_weight4_stddev',\n",
      "       'K_71_active_coef_weight4_min', 'K_72_active_coef_weight4_max'],\n",
      "      dtype='object')\n",
      "[2023-09-13 20:42:44.988543] Targets: \n",
      "[2023-09-13 20:42:44.988758] Index(['Score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "log(f\"Before preprocessing:  {train_df.shape[1] - 1} features\", logfile)\n",
    "\n",
    "# Remove constant columns\n",
    "nonconstant_columns = train_df.std() > (10 ** -10)\n",
    "train_df = train_df.loc[:, nonconstant_columns]\n",
    "valid_df = valid_df.loc[:, nonconstant_columns]\n",
    "\n",
    "# Remove correlated columns\n",
    "correlation_matrix = train_df.iloc[:,:-1].corr().abs()\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "train_df.drop(train_df[to_drop], axis=1, inplace=True)\n",
    "valid_df.drop(valid_df[to_drop], axis=1, inplace=True)\n",
    "\n",
    "log(f\"After preprocessing:  {train_df.shape[1] - 1} features\", logfile)\n",
    "log(\"Features: \",logfile)\n",
    "log(train_df.columns[:-1], logfile)\n",
    "log(\"Targets: \",logfile)\n",
    "log(train_df.columns[-1:], logfile)\n",
    "\n",
    "dataset_dir = f\"data/setcover/datasets_preprocessed/{seed}\"\n",
    "    \n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(f\"{dataset_dir}/train.csv\", index = False)\n",
    "valid_df.to_csv(f\"{dataset_dir}/valid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "branching-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
